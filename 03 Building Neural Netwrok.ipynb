{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) # 784 is 28*28 which is resolution of our images which passed which mean input can be 784 also its the same thing\n",
    "        self.fc2 = nn.Linear(64, 64) # here input will be 64 because in previous line output will be 64\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10) # output will be 10 because we have 0 to 9 handwritten number images which we want to classify\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(self.fc1(x)) # relu activation function or sigmoid to fire the neuron\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = f.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return f.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28, 28)) # Random data to ensure what type of data works on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.0291e-01, 8.1772e-01, 6.0683e-01, 6.4870e-01, 1.4800e-01, 3.5310e-01,\n",
       "         5.9021e-01, 7.5292e-01, 3.1419e-01, 3.5999e-01, 9.6108e-01, 3.7559e-01,\n",
       "         7.5186e-01, 1.0750e-01, 8.7278e-01, 5.6207e-01, 2.1081e-01, 4.3261e-01,\n",
       "         1.3367e-02, 1.1575e-01, 3.0872e-01, 8.2525e-01, 2.3747e-01, 5.9550e-02,\n",
       "         6.3400e-01, 3.8607e-01, 8.4862e-01, 4.7358e-01],\n",
       "        [3.9745e-01, 6.8379e-01, 2.9054e-01, 1.8087e-01, 3.9961e-01, 1.3052e-01,\n",
       "         3.2402e-01, 8.2545e-02, 9.5335e-02, 9.5677e-01, 9.6805e-01, 1.7118e-01,\n",
       "         5.2293e-01, 9.3132e-01, 8.0235e-01, 7.2166e-01, 3.2241e-01, 7.3643e-02,\n",
       "         6.6639e-01, 1.9105e-01, 7.3611e-01, 8.3901e-01, 1.6656e-01, 9.5851e-01,\n",
       "         8.0167e-01, 9.6781e-01, 2.0208e-02, 8.0572e-01],\n",
       "        [3.7310e-01, 9.9154e-01, 6.8466e-01, 9.7846e-01, 4.6825e-01, 8.4169e-01,\n",
       "         4.9538e-01, 5.3379e-01, 7.6193e-01, 7.1844e-01, 5.2905e-01, 9.6064e-01,\n",
       "         5.1117e-01, 6.6863e-01, 3.6796e-01, 3.1793e-01, 6.9382e-01, 6.0654e-01,\n",
       "         1.8436e-01, 8.2911e-01, 3.8500e-01, 9.4376e-01, 1.9679e-01, 1.1936e-01,\n",
       "         8.9145e-02, 2.2794e-01, 8.5952e-01, 6.9988e-02],\n",
       "        [9.2204e-01, 3.4679e-01, 2.3431e-01, 8.2587e-01, 9.3208e-01, 6.7414e-01,\n",
       "         7.1492e-01, 1.0844e-01, 4.6560e-01, 8.7143e-01, 9.1852e-01, 5.4015e-01,\n",
       "         8.9373e-01, 1.8652e-01, 7.6679e-01, 1.6833e-01, 9.4144e-01, 4.5021e-01,\n",
       "         6.4860e-01, 6.3614e-01, 5.5410e-01, 2.4234e-01, 7.4903e-01, 1.9368e-02,\n",
       "         9.3154e-01, 1.0703e-01, 4.8182e-02, 2.6265e-01],\n",
       "        [2.5380e-01, 4.1009e-01, 6.3831e-01, 1.9575e-01, 1.2137e-01, 7.2394e-02,\n",
       "         3.3890e-01, 2.7193e-02, 4.5462e-01, 8.6737e-01, 8.3183e-01, 8.1859e-01,\n",
       "         3.9208e-01, 8.2006e-02, 2.3316e-01, 9.3325e-01, 8.0255e-01, 4.6430e-01,\n",
       "         5.6956e-01, 3.4845e-01, 8.1098e-01, 1.5858e-01, 7.6864e-01, 7.9894e-01,\n",
       "         2.5164e-01, 3.8105e-01, 1.4112e-01, 2.0038e-01],\n",
       "        [6.6070e-01, 6.9247e-01, 7.9018e-01, 5.6193e-01, 5.1967e-01, 1.2744e-01,\n",
       "         2.3043e-01, 9.9583e-01, 9.7348e-01, 1.7192e-01, 5.4180e-01, 1.1307e-01,\n",
       "         7.4453e-01, 2.9302e-01, 2.3810e-02, 1.8649e-01, 5.6486e-01, 4.4493e-01,\n",
       "         8.7099e-01, 2.7389e-01, 3.8004e-01, 4.3384e-01, 3.2865e-01, 5.5641e-01,\n",
       "         5.3313e-01, 7.2799e-01, 7.8122e-01, 5.7571e-01],\n",
       "        [6.7900e-01, 1.3069e-01, 2.9870e-02, 1.3691e-02, 1.2681e-01, 4.3468e-01,\n",
       "         1.1696e-01, 3.0225e-01, 3.4397e-01, 1.8675e-01, 1.3434e-01, 2.5512e-01,\n",
       "         6.9651e-01, 8.4186e-01, 4.5834e-01, 9.8479e-01, 8.5991e-01, 4.4355e-01,\n",
       "         5.2317e-01, 8.1148e-01, 7.9489e-01, 5.4736e-01, 5.3635e-02, 4.6575e-01,\n",
       "         6.4041e-01, 1.0378e-01, 1.2415e-01, 8.6351e-01],\n",
       "        [9.4983e-01, 4.1736e-01, 4.0109e-01, 4.0855e-02, 2.1550e-01, 2.7730e-01,\n",
       "         5.8947e-01, 4.8828e-01, 8.7066e-01, 4.3020e-01, 7.8945e-01, 6.5606e-01,\n",
       "         3.1824e-01, 9.4107e-01, 7.1581e-01, 5.1804e-01, 5.1072e-01, 7.6957e-01,\n",
       "         5.1745e-01, 1.2112e-01, 6.0753e-01, 4.2803e-01, 1.9687e-01, 1.1326e-01,\n",
       "         8.0624e-01, 5.5474e-01, 3.9233e-01, 5.2784e-01],\n",
       "        [8.4311e-01, 8.6150e-01, 4.8698e-01, 4.3096e-01, 7.7469e-01, 7.4699e-01,\n",
       "         1.1701e-01, 2.6799e-01, 9.0297e-01, 4.6403e-01, 8.9869e-01, 8.9829e-01,\n",
       "         3.3904e-01, 8.2422e-01, 6.0012e-01, 5.7807e-01, 7.5886e-01, 6.5946e-01,\n",
       "         8.1519e-01, 9.2747e-01, 9.0205e-01, 8.4550e-01, 6.6685e-01, 7.1465e-02,\n",
       "         7.7568e-02, 9.8143e-01, 4.5603e-01, 5.4490e-01],\n",
       "        [6.5026e-01, 8.5690e-01, 2.7534e-01, 2.1041e-01, 3.9173e-01, 5.4181e-01,\n",
       "         3.3412e-01, 7.7889e-01, 5.7728e-01, 8.3432e-01, 7.6536e-01, 5.7068e-01,\n",
       "         4.8904e-01, 3.7560e-01, 4.3388e-01, 9.8257e-01, 4.0758e-02, 8.6282e-01,\n",
       "         7.8081e-01, 3.3943e-01, 3.2538e-01, 9.1993e-01, 1.7299e-01, 2.3205e-01,\n",
       "         3.0800e-01, 1.1369e-01, 7.2991e-01, 1.4209e-01],\n",
       "        [8.8944e-01, 1.2941e-01, 1.8830e-01, 4.3045e-01, 8.8288e-01, 8.8481e-02,\n",
       "         9.9091e-01, 6.0325e-01, 9.4604e-01, 8.3687e-02, 2.9169e-01, 6.2011e-01,\n",
       "         5.7184e-01, 5.6279e-01, 1.8984e-01, 7.4299e-01, 8.7666e-01, 5.9605e-01,\n",
       "         5.8118e-01, 1.5350e-01, 5.4935e-01, 1.4510e-01, 5.9062e-01, 1.2570e-01,\n",
       "         3.3079e-01, 4.1877e-01, 9.9151e-01, 7.1015e-01],\n",
       "        [7.5245e-01, 4.2463e-01, 9.6914e-01, 6.9994e-02, 3.9400e-01, 6.1690e-01,\n",
       "         7.4294e-01, 1.9554e-01, 1.5778e-01, 4.2885e-02, 1.1818e-01, 7.8988e-01,\n",
       "         9.5222e-01, 5.4240e-01, 4.2735e-01, 1.7513e-01, 9.3766e-01, 9.6201e-01,\n",
       "         1.1545e-01, 2.5166e-01, 8.0042e-01, 1.8582e-01, 2.8903e-01, 4.4989e-01,\n",
       "         7.8042e-01, 3.7811e-01, 4.3780e-01, 8.3059e-02],\n",
       "        [5.3641e-01, 8.4591e-01, 5.8117e-01, 7.2146e-01, 1.3880e-01, 4.1806e-01,\n",
       "         5.9826e-01, 3.5126e-01, 7.7940e-01, 6.5024e-01, 3.5736e-01, 6.6308e-01,\n",
       "         7.7956e-01, 9.1867e-01, 2.4361e-01, 4.1419e-01, 1.8520e-01, 1.1136e-01,\n",
       "         6.4915e-02, 9.0786e-01, 7.2085e-01, 5.9356e-01, 8.7070e-02, 6.3364e-01,\n",
       "         5.2174e-01, 2.8077e-01, 2.5657e-01, 6.0555e-01],\n",
       "        [5.1462e-01, 6.9664e-02, 4.0513e-01, 7.9253e-01, 3.7742e-01, 8.7414e-01,\n",
       "         3.4781e-01, 3.2849e-02, 8.4272e-01, 9.4648e-01, 6.6525e-01, 2.5553e-01,\n",
       "         2.0179e-01, 1.0247e-02, 9.3693e-01, 7.7104e-01, 2.9927e-01, 4.7090e-01,\n",
       "         4.2066e-01, 1.3537e-01, 9.8005e-01, 9.6734e-01, 3.8171e-01, 6.9227e-01,\n",
       "         2.6757e-01, 5.2507e-01, 3.1613e-01, 8.5405e-01],\n",
       "        [8.4180e-01, 8.4265e-01, 8.9673e-01, 5.6892e-02, 8.7084e-01, 8.1289e-01,\n",
       "         1.2829e-01, 4.6379e-01, 6.4699e-01, 5.2926e-01, 2.8705e-01, 1.8920e-01,\n",
       "         7.8576e-01, 5.1310e-01, 3.6591e-01, 7.8334e-01, 8.4214e-03, 1.9562e-01,\n",
       "         5.0434e-01, 4.0958e-02, 7.3372e-01, 2.3327e-01, 8.8633e-01, 9.6614e-01,\n",
       "         8.6394e-01, 7.2589e-01, 2.0250e-01, 1.9269e-01],\n",
       "        [9.9121e-01, 4.7803e-01, 2.6331e-02, 1.7370e-01, 4.8553e-01, 9.4446e-01,\n",
       "         4.3859e-01, 4.3696e-02, 1.5797e-01, 6.1090e-01, 6.6993e-02, 1.0654e-01,\n",
       "         5.2053e-01, 2.7486e-01, 4.4328e-01, 1.5259e-01, 9.0493e-01, 9.2837e-01,\n",
       "         4.1946e-02, 6.9713e-01, 4.6196e-01, 2.0200e-01, 3.0825e-01, 1.8236e-01,\n",
       "         3.7462e-01, 2.3020e-01, 7.2863e-02, 3.6229e-01],\n",
       "        [1.9594e-01, 2.7121e-01, 4.7013e-01, 9.2398e-01, 7.3655e-01, 4.6094e-02,\n",
       "         6.4404e-01, 9.8866e-01, 2.2515e-01, 9.2484e-01, 4.4752e-01, 4.0567e-01,\n",
       "         7.3636e-01, 9.8702e-01, 3.2351e-01, 1.8675e-01, 1.3275e-01, 6.5017e-02,\n",
       "         4.9906e-01, 9.8369e-01, 9.9114e-01, 4.7964e-01, 1.4403e-01, 4.2088e-01,\n",
       "         4.8116e-01, 7.1477e-01, 4.6610e-01, 1.1406e-01],\n",
       "        [2.9480e-01, 4.2626e-01, 8.7736e-01, 1.5308e-01, 5.2651e-01, 5.3302e-01,\n",
       "         3.9360e-01, 4.7509e-01, 2.7226e-01, 8.9226e-01, 6.3418e-01, 3.2158e-01,\n",
       "         1.3241e-01, 8.4619e-01, 9.4007e-01, 1.4701e-01, 8.7417e-01, 5.9017e-01,\n",
       "         6.3854e-01, 8.2088e-01, 6.4678e-01, 1.8490e-02, 4.7756e-01, 8.9764e-01,\n",
       "         3.7089e-01, 8.3972e-01, 7.8449e-01, 2.5556e-01],\n",
       "        [9.5138e-01, 2.6529e-01, 9.6042e-01, 2.3592e-01, 5.1397e-01, 9.3147e-01,\n",
       "         3.9166e-01, 4.0066e-01, 1.5016e-01, 6.0552e-01, 4.6336e-01, 5.1656e-02,\n",
       "         9.5346e-01, 8.6077e-01, 6.2364e-01, 1.1528e-01, 8.8285e-01, 3.5196e-01,\n",
       "         2.5927e-01, 5.8647e-02, 8.8948e-01, 5.7325e-01, 3.3875e-01, 7.2311e-01,\n",
       "         4.4444e-01, 1.0850e-01, 4.0769e-01, 9.6686e-02],\n",
       "        [9.5122e-01, 2.7417e-01, 1.5086e-01, 3.3376e-01, 3.9366e-01, 4.9025e-01,\n",
       "         7.8652e-01, 3.4774e-01, 2.4821e-01, 1.0154e-01, 9.8659e-01, 6.1457e-01,\n",
       "         4.6341e-01, 2.8845e-01, 9.5332e-01, 3.5410e-01, 3.1431e-01, 9.6234e-01,\n",
       "         6.8984e-01, 8.8641e-01, 5.9214e-01, 7.5833e-01, 8.3961e-02, 8.8406e-01,\n",
       "         1.0488e-01, 2.4363e-02, 6.6108e-01, 1.9908e-04],\n",
       "        [1.2800e-01, 6.0836e-01, 3.7716e-01, 5.3453e-01, 8.0815e-01, 7.2140e-01,\n",
       "         6.8462e-01, 1.3685e-01, 5.1694e-01, 7.9085e-01, 5.7571e-01, 3.8800e-01,\n",
       "         8.3646e-01, 2.4042e-02, 3.7229e-01, 3.0528e-01, 2.6982e-01, 9.3723e-02,\n",
       "         4.3269e-01, 8.7705e-01, 7.8614e-01, 5.3854e-01, 9.4158e-01, 2.7077e-02,\n",
       "         5.8902e-01, 7.9475e-01, 8.9235e-01, 4.3668e-02],\n",
       "        [3.6002e-02, 2.2763e-01, 1.6156e-01, 4.6880e-01, 5.5213e-01, 7.0630e-01,\n",
       "         6.0483e-01, 5.4848e-01, 5.3516e-01, 8.2552e-01, 9.5067e-01, 8.3531e-02,\n",
       "         2.5072e-01, 5.7926e-01, 6.7113e-01, 4.9086e-01, 9.4496e-01, 2.7427e-01,\n",
       "         4.1293e-01, 3.2998e-01, 4.9443e-01, 5.2318e-01, 4.5733e-01, 7.8119e-01,\n",
       "         6.7521e-01, 2.0195e-01, 9.8788e-01, 6.9846e-02],\n",
       "        [3.5368e-01, 1.7300e-01, 7.2025e-01, 3.8605e-01, 9.7394e-01, 7.9024e-02,\n",
       "         9.8378e-01, 3.8163e-02, 2.6378e-01, 6.0892e-01, 9.2900e-01, 6.2749e-01,\n",
       "         5.0642e-01, 6.2616e-01, 1.6075e-01, 4.7544e-01, 8.6413e-01, 6.2467e-01,\n",
       "         8.2679e-01, 7.7808e-01, 1.7872e-01, 3.9462e-01, 9.5904e-01, 3.2822e-01,\n",
       "         5.8627e-01, 3.9176e-01, 4.0464e-01, 5.6627e-01],\n",
       "        [8.4256e-01, 7.4099e-01, 7.5805e-01, 4.7203e-01, 5.7938e-01, 5.7145e-01,\n",
       "         7.2348e-01, 9.4597e-02, 7.6867e-01, 3.7065e-01, 2.9957e-02, 1.1535e-01,\n",
       "         8.0681e-01, 6.2840e-01, 7.5416e-01, 6.5974e-01, 6.7778e-01, 1.9069e-01,\n",
       "         6.6206e-01, 9.1015e-01, 6.4330e-01, 2.1251e-01, 2.7256e-01, 5.6626e-02,\n",
       "         2.7690e-01, 3.4528e-01, 8.1621e-01, 4.8159e-01],\n",
       "        [2.6731e-01, 7.0518e-01, 5.7366e-01, 2.2456e-01, 5.6720e-01, 8.4296e-01,\n",
       "         4.0454e-01, 3.4528e-01, 4.2527e-01, 7.8448e-01, 9.6565e-02, 9.3033e-01,\n",
       "         9.8320e-01, 5.3777e-01, 8.1626e-02, 9.2755e-01, 9.6633e-02, 7.2168e-01,\n",
       "         7.9209e-02, 5.6626e-01, 3.8499e-01, 3.7330e-01, 5.7485e-01, 5.0277e-04,\n",
       "         2.0333e-01, 8.8982e-01, 7.5820e-01, 5.3018e-01],\n",
       "        [4.6122e-01, 9.5609e-01, 7.4347e-01, 5.3393e-01, 7.2196e-01, 3.4403e-01,\n",
       "         7.0890e-01, 1.3743e-01, 6.4146e-01, 8.3673e-01, 9.1907e-01, 1.6613e-01,\n",
       "         5.6891e-02, 3.4079e-01, 6.7303e-01, 2.3002e-01, 8.0445e-01, 4.3402e-01,\n",
       "         1.6626e-02, 9.4787e-01, 4.8246e-01, 7.9232e-01, 8.6303e-01, 6.7743e-01,\n",
       "         9.7070e-01, 7.4957e-02, 4.3904e-01, 3.9462e-01],\n",
       "        [5.1777e-01, 1.7626e-02, 6.8717e-01, 6.7478e-01, 4.1199e-01, 1.4067e-01,\n",
       "         3.1021e-01, 7.8568e-01, 3.6903e-01, 4.7318e-01, 4.6821e-01, 1.7639e-02,\n",
       "         1.3639e-01, 4.9648e-01, 2.6416e-01, 5.9279e-01, 8.6997e-01, 4.8232e-02,\n",
       "         9.4547e-01, 2.6208e-01, 9.0228e-02, 4.1939e-01, 9.4321e-02, 3.9089e-01,\n",
       "         2.6843e-01, 5.0667e-01, 8.8024e-02, 4.3138e-01],\n",
       "        [2.7321e-02, 3.2368e-01, 9.4986e-01, 9.7977e-01, 8.7940e-01, 7.0017e-01,\n",
       "         6.2670e-01, 7.4987e-01, 7.8483e-01, 7.1590e-01, 5.1452e-01, 1.0442e-01,\n",
       "         4.4811e-01, 4.8158e-01, 7.6189e-01, 3.7955e-01, 4.7430e-01, 4.3000e-01,\n",
       "         4.7392e-01, 8.8104e-01, 3.1236e-01, 1.9884e-01, 8.3774e-02, 2.9084e-01,\n",
       "         1.9141e-01, 5.0566e-01, 1.3603e-02, 8.8414e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# model says the data is not 28x28\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# relu activation function or sigmoid to fire the neuron\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x64)"
     ]
    }
   ],
   "source": [
    "output = net(X) # model says the data is not 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# even then dimension out of range because expected data is -1 to 0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1974\u001b[0m, in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1972\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1974\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1976\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "X = X.view(28*28)\n",
    "output = net(X) # even then dimension out of range because expected data is -1 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3959, -2.2320, -2.3539, -2.2963, -2.3794, -2.1874, -2.1816, -2.3702,\n",
       "         -2.2582, -2.4047]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.view(-1, 28*28)\n",
    "output = net(X) # it works because all in minus or all divided by 1 so it doesnt matter infinity negative but not greater than 1\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
